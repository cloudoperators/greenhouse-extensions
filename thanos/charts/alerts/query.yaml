groups:
- name: thanos-query
  rules:
  - alert: ThanosQueryHttpRequestQueryErrorRateHigh
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` is failing to handle `{{`{{$value | humanize}}`}}`%
        of "query" requests.
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(http_requests_total{code=~"5..", job=~".*query.*", handler="query"}[5m]))
      /
        sum by (job) (rate(http_requests_total{job=~".*query.*", handler="query"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryerrorratehigh
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryHttpRequestQueryRangeErrorRateHigh
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` is failing to handle `{{`{{$value | humanize}}`}}`%
        of "query_range" requests.
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(http_requests_total{code=~"5..", job=~".*query.*", handler="query_range"}[5m]))
      /
        sum by (job) (rate(http_requests_total{job=~".*query.*", handler="query_range"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhttprequestqueryrangeerrorratehigh
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryGrpcServerErrorRate
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` is failing to handle `{{`{{$value | humanize}}`}}`%
        of requests.
      summary: Thanos Query is failing to handle requests.
    expr: |
      (
        sum by (job) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*query.*"}[5m]))
      /
        sum by (job) (rate(grpc_server_started_total{job=~".*query.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcservererrorrate
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryGrpcClientErrorRate
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` is failing to send `{{`{{$value | humanize}}`}}`%
        of requests.
      summary: Thanos Query is failing to send requests.
    expr: |
      (
        sum by (job) (rate(grpc_client_handled_total{grpc_code!="OK", job=~".*query.*"}[5m]))
      /
        sum by (job) (rate(grpc_client_started_total{job=~".*query.*"}[5m]))
      ) * 100 > 5
    for: 5m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosquerygrpcclienterrorrate
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryHighDNSFailures
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` have `{{`{{$value | humanize}}`}}`% of failing
        DNS queries for store endpoints.
      summary: Thanos Query is having high number of DNS failures.
    expr: |
      (
        sum by (job) (rate(thanos_query_endpoints_dns_failures_total{job=~".*query.*"}[5m]))
      /
        sum by (job) (rate(thanos_query_endpoints_dns_lookups_total{job=~".*query.*"}[5m]))
      ) * 100 > 1
    for: 15m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryhighdnsfailures
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryInstantLatencyHigh
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` has a 99th percentile latency of `{{`{{$value}}`}}`
        seconds for instant queries.
      summary: Thanos Query has high latency for queries.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*query.*", handler="query"}[5m]))) > 40
      and
        sum by (job) (rate(http_request_duration_seconds_bucket{job=~".*query.*", handler="query"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryinstantlatencyhigh
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryRangeLatencyHigh
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` has a 99th percentile latency of `{{`{{$value}}`}}`
        seconds for range queries.
      summary: Thanos Query has high latency for queries.
    expr: |
      (
        histogram_quantile(0.99, sum by (job, le) (rate(http_request_duration_seconds_bucket{job=~".*query.*", handler="query_range"}[5m]))) > 90
      and
        sum by (job) (rate(http_request_duration_seconds_count{job=~".*query.*", handler="query_range"}[5m])) > 0
      )
    for: 10m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryrangelatencyhigh
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryOverload
    annotations:
      description: Thanos Query `{{`{{$labels.job}}`}}` has been overloaded for more than
        15 minutes. This may be a symptom of excessive simultaneous complex requests,
        low performance of the Prometheus API, or failures within these components.
        Assess the health of the Thanos query instances, the connected Prometheus
        instances, look for potential senders of these requests and then contact support.
      summary: Thanos query reaches its maximum capacity serving concurrent requests.
    expr: |
      (
        max_over_time(thanos_query_concurrent_gate_queries_max[5m]) - avg_over_time(thanos_query_concurrent_gate_queries_in_flight[5m]) < 1
      )
    for: 15m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryoverload
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosQueryIsDown
    annotations:
      description: ThanosQuery has disappeared. Prometheus target for the component
        cannot be discovered.
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*query.*"} == 1)
    for: 10m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosqueryisdown
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
