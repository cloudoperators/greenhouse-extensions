groups:
- name: thanos-ruler
  rules:
  - alert: ThanosRuleQueueIsDroppingAlerts
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` is failing to queue alerts.
      summary: Thanos Rule is failing to queue alerts.
    expr: |
      sum by (job, instance) (rate(thanos_alert_queue_alerts_dropped_total{job=~".*ruler.*"}[5m])) > 0
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeueisdroppingalerts
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleSenderIsFailingAlerts
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` is failing to send alerts to alertmanager.
      summary: Thanos Rule is failing to send alerts to alertmanager.
    expr: |
      sum by (job, instance) (rate(thanos_alert_sender_alerts_dropped_total{job=~".*ruler.*"}[5m])) > 0
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulesenderisfailingalerts
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleHighRuleEvaluationFailures
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` is failing to evaluate rules.
      summary: Thanos Rule is failing to evaluate rules.
    expr: |
      (
        sum by (job, instance) (rate(prometheus_rule_evaluation_failures_total{job=~".*ruler.*"}[5m]))
      /
        sum by (job, instance) (rate(prometheus_rule_evaluations_total{job=~".*ruler.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationfailures
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleHighRuleEvaluationWarnings
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` has high number of evaluation
        warnings.
      summary: Thanos Rule has high number of evaluation warnings.
    expr: |
      sum by (job, instance) (rate(thanos_rule_evaluation_with_warnings_total{job=~".*ruler.*"}[5m])) > 0
    for: 15m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulehighruleevaluationwarnings
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleRuleEvaluationLatencyHigh
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` has higher evaluation latency
        than interval for `{{`{{$labels.rule_group}}`}}`.
      summary: Thanos Rule has high rule evaluation latency.
    expr: |
      (
        sum by (job, instance, rule_group) (prometheus_rule_group_last_duration_seconds{job=~".*ruler.*"})
      >
        sum by (job, instance, rule_group) (prometheus_rule_group_interval_seconds{job=~".*ruler.*"})
      )
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleruleevaluationlatencyhigh
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleGrpcErrorRate
    annotations:
      description: Thanos Rule `{{`{{$labels.job}}`}}` is failing to handle `{{`{{$value | humanize}}`}}`%
        of requests.
      summary: Thanos Rule is failing to handle grpc requests.
    expr: |
      (
        sum by (job, instance) (rate(grpc_server_handled_total{grpc_code=~"Unknown|ResourceExhausted|Internal|Unavailable|DataLoss|DeadlineExceeded", job=~".*ruler.*"}[5m]))
      /
        sum by (job, instance) (rate(grpc_server_started_total{job=~".*ruler.*"}[5m]))
      * 100 > 5
      )
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulegrpcerrorrate
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleConfigReloadFailure
    annotations:
      description: Thanos Rule `{{`{{$labels.job}}`}}` has not been able to reload its configuration.
      summary: Thanos Rule has not been able to reload configuration.
    expr: avg by (job, instance) (thanos_rule_config_last_reload_successful{job=~".*ruler.*"})
      != 1
    for: 5m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleconfigreloadfailure
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleQueryHighDNSFailures
    annotations:
      description: Thanos Rule `{{`{{$labels.job}}`}}` has `{{`{{$value | humanize}}`}}`% of failing
        DNS queries for query endpoints.
      summary: Thanos Rule is having high number of DNS failures.
    expr: |
      (
        sum by (job, instance) (rate(thanos_rule_query_apis_dns_failures_total{job=~".*ruler.*"}[5m]))
      /
        sum by (job, instance) (rate(thanos_rule_query_apis_dns_lookups_total{job=~".*ruler.*"}[5m]))
      * 100 > 1
      )
    for: 15m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulequeryhighdnsfailures
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleAlertmanagerHighDNSFailures
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` has `{{`{{$value | humanize}}`}}`% of
        failing DNS queries for Alertmanager endpoints.
      summary: Thanos Rule is having high number of DNS failures.
    expr: |
      (
        sum by (job, instance) (rate(thanos_rule_alertmanagers_dns_failures_total{job=~".*ruler.*"}[5m]))
      /
        sum by (job, instance) (rate(thanos_rule_alertmanagers_dns_lookups_total{job=~".*ruler.*"}[5m]))
      * 100 > 1
      )
    for: 15m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulealertmanagerhighdnsfailures
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleNoEvaluationFor10Intervals
    annotations:
      description: Thanos Rule `{{`{{$labels.job}}`}}` has rule groups that did not evaluate
        for at least 10x of their expected interval.
      summary: Thanos Rule has rule groups that did not evaluate for 10 intervals.
    expr: |
      time() -  max by (job, instance, group) (prometheus_rule_group_last_evaluation_timestamp_seconds{job=~".*ruler.*"})
      >
      10 * max by (job, instance, group) (prometheus_rule_group_interval_seconds{job=~".*ruler.*"})
    for: 5m
    labels:
      severity: info
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosrulenoevaluationfor10intervals
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosNoRuleEvaluations
    annotations:
      description: Thanos Rule `{{`{{$labels.instance}}`}}` did not perform any rule evaluations
        in the past 10 minutes.
      summary: Thanos Rule did not perform any rule evaluations.
    expr: |
      sum by (job, instance) (rate(prometheus_rule_evaluations_total{job=~".*ruler.*"}[5m])) <= 0
        and
      sum by (job, instance) (thanos_rule_loaded_rules{job=~".*ruler.*"}) > 0
    for: 5m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosnoruleevaluations
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
  - alert: ThanosRuleIsDown
    annotations:
      description: ThanosRule has disappeared. Prometheus target for the component
        cannot be discovered.
      summary: Thanos component has disappeared.
    expr: |
      absent(up{job=~".*ruler.*"} == 1)
    for: 10m
    labels:
      severity: warning
      playbook: https://github.com/thanos-io/thanos/tree/main/mixin/runbook.md#alert-name-thanosruleisdown
      {{ tpl .Values.thanos.serviceMonitor.alertLabels . | nindent 6 }}
