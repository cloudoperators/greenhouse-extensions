groups:
  - name: collector-alerts
    rules:
  {{- if not (has "FilelogRefusedLogs" .Values.openTelemetry.prometheus.rules.collector.disabled) }}
      - alert: FilelogRefusedLogs
        expr: sum(rate(otelcol_receiver_refused_log_records_total{receiver=~"filelog"}[1m])) > 0
        for: 5m
        labels:
          severity: warning
          playbook: https://github.com/cloudoperators/greenhouse-extensions/tree/main/logs/playbooks/FilelogRefusedLogs.md
          {{- include "plugin.additionalRuleLabels" . | nindent 10 }}
        annotations:
          summary: Logs are not successfully pushed into the filelog-receiver
          description: Filelog receiver is increasingly rejecting logs
  {{- end }}

  {{- if not (has "ReceiverRefusedMetric" .Values.openTelemetry.prometheus.rules.collector.disabled) }}
      - alert: ReceiverRefusedMetric
        expr: sum(rate(otelcol_receiver_refused_metric_points_total{}[1m])) > 0
        for: 5m
        labels:
          severity: warning
          playbook: https://github.com/cloudoperators/greenhouse-extensions/tree/main/logs/playbooks/ReceiverRefusedMetric.md
          {{- include "plugin.additionalRuleLabels" . | nindent 10 }}
        annotations:
          summary: OTel is refusing metric points
          description: The OTel Collector has refused metric points for over 5 minutes. This may indicate malformed metrics. Review configuration and incoming traffic for issues.
  {{- end }}

  {{- if not (has "LogsOTelLogsMissing" .Values.openTelemetry.prometheus.rules.collector.disabled) }}
      - alert: LogsOTelLogsMissing
        expr: sum by (region, k8s_node_name) (rate(otelcol_exporter_sent_log_records_total{job=~".*/opentelemetry-collector-logs", exporter !~"debug"}[60m])) == 0
        for: 120m
        labels:
          severity: warning
          playbook: https://github.com/cloudoperators/greenhouse-extensions/tree/main/logs/playbooks/OTelLogsMissing.md
          {{- include "plugin.additionalRuleLabels" . | nindent 10 }}
        annotations:
          summary: OTel is not shipping logs
          description: 'otel-logs on {{`{{ $labels.k8s_node_name }}`}} in {{`{{ $labels.region }}`}} is not shipping logs. Please check.'
  {{- end }}

 {{- if not (has "LogsOTelLogsDecreasing" .Values.openTelemetry.prometheus.rules.collector.disabled) }}
      - alert: LogsOTelLogsDecreasing
        expr: sum(rate(otelcol_exporter_sent_log_records_total{job="logs/opentelemetry-collector-logs"}[1h]offset 2h)) by (k8s_cluster_name)/sum(rate(otelcol_exporter_sent_log_records_total{job="logs/opentelemetry-collector-logs"}[1h])) by (k8s_cluster_name) > 4
        for: 2h
        labels:
          severity: warning
          playbook: https://github.com/cloudoperators/greenhouse-extensions/tree/main/logs/playbooks/LogsOTelLogsDecreasing.md
          {{- include "plugin.additionalRuleLabels" . | nindent 10 }}
        annotations:
          summary: OTel log volume is decreasing, check log volume.
          description: 'OTel on {{`{{ $labels.k8s_cluster_name }}`}} in {{`{{ $labels.region }}`}} is sending 4 times fewer logs in the last 2h. Please check.'
  {{- end }}
