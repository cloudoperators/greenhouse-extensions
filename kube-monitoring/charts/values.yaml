global:

  # --  Labels to add to all resources. 
  # This can be used to add a support_group or service label to all resources and alerting rules.
  # @section -- Global Configuration
  commonLabels: {}

## kube-prometheus-stack configuration scoped to kube-monitoring
kubeMonitoring:

  ## Create open source community rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      # -- Enable alertmanager rules for monitoring Alertmanager
      # @section -- Default Rules
      alertmanager: false
      # -- Enable etcd rules for monitoring etcd
      # @section -- Default Rules
      etcd: false
      # -- Enable rules for monitoring Prometheus config reloaders
      # @section -- Default Rules
      configReloaders: false
      # -- Enable general rules for cluster monitoring
      # @section -- Default Rules
      general: false
      # -- Enable rules for monitoring container CPU usage
      # @section -- Default Rules
      k8sContainerCpuUsageSecondsTotal: false
      # -- Enable rules for monitoring container memory cache usage
      # @section -- Default Rules
      k8sContainerMemoryCache: false
      # -- Enable rules for monitoring container memory RSS usage
      # @section -- Default Rules
      k8sContainerMemoryRss: false
      # -- Enable rules for monitoring container memory swap usage
      # @section -- Default Rules
      k8sContainerMemorySwap: false
      # -- Enable rules for monitoring container resource usage
      # @section -- Default Rules
      k8sContainerResource: false
      # -- Enable rules for monitoring container memory working set bytes
      # @section -- Default Rules
      k8sContainerMemoryWorkingSetBytes: false
      # -- Enable rules for monitoring pod owner relationships
      # @section -- Default Rules
      k8sPodOwner: false
      # -- Enable rules for monitoring API server availability
      # @section -- Default Rules
      kubeApiserverAvailability: false
      # -- Enable rules for monitoring API server burn rate
      # @section -- Default Rules
      kubeApiserverBurnrate: false
      # -- Enable histogram rules for API server
      # @section -- Default Rules
      kubeApiserverHistogram: false
      # -- Enable SLO rules for API server
      # @section -- Default Rules
      kubeApiserverSlos: false
      # -- Enable rules for monitoring the kube-controller-manager
      # @section -- Default Rules
      kubeControllerManager: false
      # -- Enable rules for monitoring kubelet
      # @section -- Default Rules
      kubelet: false
      # -- Enable rules for monitoring kube-proxy
      # @section -- Default Rules
      kubeProxy: false
      # -- Enable general rules for kube-prometheus
      # @section -- Default Rules
      kubePrometheusGeneral: false
      # -- Enable node recording rules for kube-prometheus
      # @section -- Default Rules
      kubePrometheusNodeRecording: false
      # -- Enable rules for monitoring Kubernetes applications
      # @section -- Default Rules
      kubernetesApps: false
      # -- Enable rules for monitoring Kubernetes resources
      # @section -- Default Rules
      kubernetesResources: false
      # -- Enable rules for monitoring Kubernetes storage
      # @section -- Default Rules
      kubernetesStorage: false
      # -- Enable rules for monitoring Kubernetes system components
      # @section -- Default Rules
      kubernetesSystem: false
      # -- Enable alerting rules for kube-scheduler
      # @section -- Default Rules
      kubeSchedulerAlerting: false
      # -- Enable recording rules for kube-scheduler
      # @section -- Default Rules
      kubeSchedulerRecording: false
      # -- Enable rules for monitoring kube-state-metrics
      # @section -- Default Rules
      kubeStateMetrics: false
      # -- Enable rules for monitoring network-related metrics
      # @section -- Default Rules
      network: false
      # -- Enable rules for monitoring node-related metrics
      # @section -- Default Rules
      node: false
      # -- Enable alerting rules for node-exporter
      # @section -- Default Rules
      nodeExporterAlerting: false
      # -- Enable recording rules for node-exporter
      # @section -- Default Rules
      nodeExporterRecording: false
      # -- Enable rules for monitoring Windows-specific metrics
      # @section -- Default Rules
      windows: false

      # -- Enable useful alerting rules for self-monitoring Prometheus
      # @section -- Default Rules
      prometheus: true
      # -- Enable useful alerting rules for self-monitoring Prometheus Operator
      # @section -- Default Rules
      prometheusOperator: true

    # -- Additional labels for PrometheusRule alerts. E.g. support_group and service.
    # @section -- Default Rules
    additionalRuleLabels: {}

  ## Create curated dashboards for generic monitoring (e.g. monitoring Prometheus)
  ##
  dashboards:
    create: true

    ## Label selectors for the Plutono dashboards to be picked up by Plutono.
    plutonoSelectors:
      - name: plutono-dashboard
        value: '"true"'

  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true

  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: true

  windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
    enabled: false

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: false

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: false

  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  ##
  thanosRuler:

    ## Deploy thanosRuler
    ##
    enabled: false

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: false

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

  kubelet:
    enabled: true

    serviceMonitor:
      cAdvisorMetricRelabelings:
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)'
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
        # Additional relabeling to make the cAdvisor metrics more useful.
        - sourceLabels: [id]
          action: replace
          regex: '^/system\.slice/(.+)\.service$'
          targetLabel: systemd_service_name
          replacement: '${1}'
        - sourceLabels: [container]
          regex: '^$'
          action: drop
        - regex: '^id$'
          action: labeldrop
        - regex: '^name$'
          action: labeldrop

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    prometheus:
      monitor:
        relabelings:
          - sourceLabels: [__meta_kubernetes_pod_node_name]
            separator: ;
            regex: ^(.*)$
            targetLabel: node
            replacement: $1
            action: replace

    extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tmpfs|tracefs)$$
      - --collector.systemd.enable-task-metrics
      - --collector.systemd.enable-restarts-metrics
      - --collector.systemd.enable-start-time-metrics
      - --collector.processes
      - --collector.mountstats

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    metricLabelsAllowlist:
      - cronjobs=[*]
      - daemonsets=[*]
      - deployments=[*]
      - endpoints=[*]
      - ingresses=[*]
      - jobs=[*]
      - namespaces=[*]
      - nodes=[*]
      - persistentvolumes=[*]
      - persistentvolumeclaims=[*]
      - pods=[*]
      - secrets=[*]
      - services=[*]
      - statefulsets=[*]

  ## Service Discovery Configuration
  ##
  serviceDiscovery:


    pods:
      # -- Enable pod discovery
      # @section -- Service Discovery
      enabled: true

      # -- To avoid multiple pod scrapes from different Prometheis, service discovery can be limited to one target
      # @section -- Service Discovery
      limitToPrometheusTargets: true

      # -- Monitor Pods with the following port name
      # @section -- Service Discovery
      port: metrics

      #-- RelabelConfigs to apply to samples before scraping
      # ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      # @section -- Service Discovery
      additionalRelabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      # -- MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      # @section -- Service Discovery
      additionalMetricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      # --  Pod label for use in assembling a job name of the form <label value>-<port>
      #  If no label is specified, the pod endpoint name is used.
      # @section -- Service Discovery
      jobLabel: ""

      # -- Namespaces from which pods are selected
      # @section -- Service Discovery
      namespaceSelector:
        ## Match any namespace
        ##
        any: true

        ## Explicit list of namespace names to select
        ##
        # matchNames: []

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      # sampleLimit: 0

  ## Configuration for the Prometheus instance
  ##
  prometheus:
    service:
      # -- Add the label to expose the Prometheus service via Greenhouse.
      # @section -- Prometheus
      labels:
        greenhouse.sap/expose: "true"

    ingress:
      # --  Deploy Prometheus Ingess
      # @section -- Prometheus
      enabled: false

      ## By default, the alerts plugin deploys a ca-bundle to enable tls between Prometheus and Alertmanager
      annotations:
        disco: "true"
        kubernetes.io/tls-acme: "true"
        nginx.ingress.kubernetes.io/auth-tls-secret: "{{ $.Release.Namespace }}/{{ $.Release.Namespace }}-ca-bundle"
        nginx.ingress.kubernetes.io/auth-tls-verify-client: "true"
        nginx.ingress.kubernetes.io/auth-tls-verify-depth: "3"
      # -- Specifies the ingress-controller
      # @section -- Prometheus
      ingressClassName: nginx

    # -- Deploys a Plutono datasource config Secret for this Prometheus
    # @section -- Prometheus
    plutonoDatasource: true

    # -- Deploys a Perses datasource ConfigMap for this Prometheus
    # @section -- Prometheus
    persesDatasource: true

    prometheusSpec:

      ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertManagerConfigs
      additionalAlertManagerConfigsSecret:
        name: "{{ $.Release.Name }}-alertmanager-config"
        key: config.yaml

      ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertRelabelConfigs
      additionalAlertRelabelConfigsSecret:
        name: "{{ $.Release.Name }}-alertmanager-config"
        key: relabelConfig.yaml

      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
      ## with the new list of secrets.
      secrets:
        - "tls-prometheus-{{ .Release.Name }}"

      storageSpec:
        volumeClaimTemplate:
          metadata:
            labels:
              plugindefinition: kube-monitoring
              plugin: "{{ $.Release.Name }}"
              app: "{{ $.Release.Name }}-prometheus"
            name: "{{ $.Release.Name }}"
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                  # -- How large the persistent volume should be to house the prometheus database.
                  # @section -- Prometheus
                storage: 50Gi

      ## How long to retain metrics
      ## retention: 10d

      # -- ServiceMonitors to be selected for target discovery.
      # If {}, it will select all ServiceMonitors
      # @section -- Prometheus
      serviceMonitorSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      # -- PodMonitors to be selected for target discovery.
      # If {}, it will select all PodMonitors
      # @section -- Prometheus
      podMonitorSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      # -- Probes to be selected for target discovery.
      # If {}, it will select all Probes
      # @section -- Prometheus
      probeSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      # -- scrapeConfigs to be selected for target discovery.
      # If {}, it will select all scrapeConfigs
      # @section -- Prometheus
      scrapeConfigSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      # -- PrometheusRules to be selected for target discovery.
      # If {}, it will select all PrometheusRules
      # @section -- Prometheus
      ruleSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"

kubernetes-operations:
  prometheusRules:
    ruleSelector:
      - name: plugin
        value: "{{ $.Release.Name }}"

## Configures Prometheus Alertmanager
alerts:
  # -- Enable Alertmanager
  # @section -- Alertmanager
  enabled: false
  alertmanagers:
    # -- List of Alertmanager hosts Prometheus can send alerts to
    # @section -- Alertmanager
    hosts: []
    # -- Overrides tls certificate to authenticate with Alertmanager
    # @section -- Alertmanager
    tlsConfig:
      # -- TLS certificate for communication with Alertmanager
      # @section -- Alertmanager
      cert: ""
      # -- TLS key for communication with Alertmanager
      # @section -- Alertmanager
      key: ""

testFramework:
  enabled: true
  image:
    registry: ghcr.io
    repository: cloudoperators/greenhouse-extensions-integration-test
    tag: main
  imagePullPolicy: IfNotPresent

blackboxExporter:
  enabled: false
