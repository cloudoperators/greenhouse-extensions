global:

  ## Labels to apply to all resources
  ##
  commonLabels: {}

## kube-prometheus-stack configuration scoped to kube-monitoring
kubeMonitoring:

  ## Create open source community rules for monitoring the cluster
  ##
  defaultRules:
    create: true
    rules:
      alertmanager: false
      etcd: false
      configReloaders: false
      general: false
      k8sContainerCpuUsageSecondsTotal: false
      k8sContainerMemoryCache: false
      k8sContainerMemoryRss: false
      k8sContainerMemorySwap: false
      k8sContainerResource: false
      k8sContainerMemoryWorkingSetBytes: false
      k8sPodOwner: false
      kubeApiserverAvailability: false
      kubeApiserverBurnrate: false
      kubeApiserverHistogram: false
      kubeApiserverSlos: false
      kubeControllerManager: false
      kubelet: false
      kubeProxy: false
      kubePrometheusGeneral: false
      kubePrometheusNodeRecording: false
      kubernetesApps: false
      kubernetesResources: false
      kubernetesStorage: false
      kubernetesSystem: false
      kubeSchedulerAlerting: false
      kubeSchedulerRecording: false
      kubeStateMetrics: false
      network: false
      node: false
      nodeExporterAlerting: false
      nodeExporterRecording: false
      windows: false

      ## Enable useful alerting rules for self-monitoring Prometheus
      prometheus: true
      prometheusOperator: true

    ## Additional labels for PrometheusRule alerts. E.g. support_group and service.
    additionalRuleLabels: {}

  ## Create curated dashboards for generic monitoring (e.g. monitoring Prometheus)
  ##
  dashboards:
    create: true

    ## Label selectors for the Plutono dashboards to be picked up by Plutono.
    plutonoSelectors:
      - name: plutono-dashboard
        value: '"true"'

  ## Install Prometheus Operator CRDs
  ##
  crds:
    enabled: true

  ## Setting to true produces cleaner resource names, but requires a data migration because the name of the persistent volume changes. Therefore this should only be set once on initial installation.
  ##
  cleanPrometheusOperatorObjectNames: true

  windowsMonitoring:
  ## Deploys the windows-exporter and Windows-specific dashboards and rules (job name must be 'windows-exporter')
    enabled: false

  ## Configuration for alertmanager
  ## ref: https://prometheus.io/docs/alerting/alertmanager/
  ##
  alertmanager:

    ## Deploy alertmanager
    ##
    enabled: false

  ## Using default values from https://github.com/grafana/helm-charts/blob/main/charts/grafana/values.yaml
  ##
  grafana:
    enabled: false

  ## Configuration for thanosRuler
  ## ref: https://thanos.io/tip/components/rule.md/
  ##
  thanosRuler:

    ## Deploy thanosRuler
    ##
    enabled: false

  ## Component scraping the kube controller manager
  ##
  kubeControllerManager:
    enabled: false

  ## Component scraping kube proxy
  ##
  kubeProxy:
    enabled: false

  ## Component scraping kube scheduler
  ##
  kubeScheduler:
    enabled: false

  kubelet:
    enabled: true

    serviceMonitor:
      cAdvisorMetricRelabelings:
        # Drop less useful container CPU metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_cpu_(cfs_throttled_seconds_total|load_average_10s|system_seconds_total|user_seconds_total)'
        # Drop less useful container / always zero filesystem metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_fs_(io_current|io_time_seconds_total|io_time_weighted_seconds_total|reads_merged_total|sector_reads_total|sector_writes_total|writes_merged_total)'
        # Drop less useful / always zero container memory metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_memory_(mapped_file|swap)'
        # Drop less useful container process metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_(file_descriptors|tasks_state|threads_max)'
        # Drop container spec metrics that overlap with kube-state-metrics.
        - sourceLabels: [__name__]
          action: drop
          regex: 'container_spec.*'
        # Drop cgroup metrics with no pod.
        - sourceLabels: [id, pod]
          action: drop
          regex: '.+;'
        # Additional relabeling to make the cAdvisor metrics more useful.
        - sourceLabels: [id]
          action: replace
          regex: '^/system\.slice/(.+)\.service$'
          targetLabel: systemd_service_name
          replacement: '${1}'
        - sourceLabels: [container]
          regex: '^$'
          action: drop
        - regex: '^id$'
          action: labeldrop
        - regex: '^name$'
          action: labeldrop

  ## Configuration for prometheus-node-exporter subchart
  ##
  prometheus-node-exporter:
    prometheus:
      monitor:
        relabelings:
          - sourceLabels: [__meta_kubernetes_pod_node_name]
            separator: ;
            regex: ^(.*)$
            targetLabel: node
            replacement: $1
            action: replace

    extraArgs:
      - --collector.filesystem.ignored-mount-points=^/(sys|proc|dev|host|etc)($$|/)
      - --collector.filesystem.ignored-fs-types=^(autofs|binfmt_misc|bpf|cgroup|configfs|debugfs|devpts|devtmpfs|fusectl|hugetlbfs|mqueue|nsfs|overlay|proc|procfs|pstore|rpc_pipefs|securityfs|selinuxfs|squashfs|sysfs|tmpfs|tracefs)$$
      - --collector.systemd.enable-task-metrics
      - --collector.systemd.enable-restarts-metrics
      - --collector.systemd.enable-start-time-metrics
      - --collector.processes
      - --collector.mountstats

  ## Configuration for kube-state-metrics subchart
  ##
  kube-state-metrics:
    metricLabelsAllowlist:
      - cronjobs=[*]
      - daemonsets=[*]
      - deployments=[*]
      - endpoints=[*]
      - ingresses=[*]
      - jobs=[*]
      - namespaces=[*]
      - nodes=[*]
      - persistentvolumes=[*]
      - persistentvolumeclaims=[*]
      - pods=[*]
      - secrets=[*]
      - services=[*]
      - statefulsets=[*]

  ## Service discovery configuration
  ##
  serviceDiscovery:

    ## Configuration for the Prometheus Operator to discover pods
    ##
    pods:
      enabled: true

      ## To avoid multiple pod scrapes from different Prometheis, service discovery can be limited to one target
      limitToPrometheusTargets: true

      ## Monitor Pods with the following port name
      port: metrics

      ## RelabelConfigs to apply to samples before scraping
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      additionalRelabelings: []
      # - sourceLabels: [__meta_kubernetes_pod_node_name]
      #   separator: ;
      #   regex: ^(.*)$
      #   targetLabel: nodename
      #   replacement: $1
      #   action: replace

      ## MetricRelabelConfigs to apply to samples after scraping, but before ingestion.
      ## ref: https://github.com/prometheus-operator/prometheus-operator/blob/main/Documentation/api.md#relabelconfig
      ##
      additionalMetricRelabelings: []
      # - action: keep
      #   regex: 'kube_(daemonset|deployment|pod|namespace|node|statefulset).+'
      #   sourceLabels: [__name__]

      ## Pod label for use in assembling a job name of the form <label value>-<port>
      ## If no label is specified, the pod endpoint name is used.
      ##
      jobLabel: ""

      ## Namespaces from which pods are selected
      ##
      namespaceSelector:
        ## Match any namespace
        ##
        any: true

        ## Explicit list of namespace names to select
        ##
        # matchNames: []

      ## SampleLimit defines per-scrape limit on number of scraped samples that will be accepted.
      ##
      # sampleLimit: 0

  ## Configuration for the Prometheus instance
  ##
  prometheus:
    service:
      # Add the label to expose the service via Greenhouse.
      labels:
        greenhouse.sap/expose: "true"

    ingress:
      enabled: false

      ## By default, a ca-bundle is deployed to enable tls between Prometheus and Alertmanager
      annotations:
        disco: "true"
        kubernetes.io/tls-acme: "true"
        nginx.ingress.kubernetes.io/auth-tls-secret: "{{ $.Release.Namespace }}/{{ $.Release.Namespace }}-ca-bundle"
        nginx.ingress.kubernetes.io/auth-tls-verify-client: "true"
        nginx.ingress.kubernetes.io/auth-tls-verify-depth: "3"
      ingressClassName: nginx

    # Deploys a Plutono datasource config Secret for this Prometheus
    plutonoDatasource: true

    # Deploys a Perses datasource ConfigMap for this Prometheus
    persesDatasource: true

    prometheusSpec:

      ## If additional alertmanager configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertManagerConfigs
      additionalAlertManagerConfigsSecret:
        name: kube-monitoring-alertmanager-config
        key: config.yaml

      ## If additional alert relabel configurations are already deployed in a single secret, or you want to manage
      ## them separately from the helm deployment, you can use this section.
      ## Expected values are the secret name and key
      ## Cannot be used with additionalAlertRelabelConfigs
      additionalAlertRelabelConfigsSecret:
        name: kube-monitoring-alertmanager-config
        key: relabelConfig.yaml

      ## Secrets is a list of Secrets in the same namespace as the Prometheus object, which shall be mounted into the Prometheus Pods.
      ## The Secrets are mounted into /etc/prometheus/secrets/. Secrets changes after initial creation of a Prometheus object are not
      ## reflected in the running Pods. To change the secrets mounted into the Prometheus Pods, the object must be deleted and recreated
      ## with the new list of secrets.
      secrets:
        - tls-prometheus-alertmanager-auth

      storageSpec:
        volumeClaimTemplate:
          metadata:
            labels:
              plugindefinition: kube-monitoring
              plugin: "{{ $.Release.Name }}"
              app: "{{ $.Release.Name }}-prometheus"
            name: "{{ $.Release.Name }}"
          spec:
            accessModes:
              - ReadWriteOnce
            resources:
              requests:
                storage: 50Gi

      ## ServiceMonitors to be selected for target discovery.
      ## If {}, select all ServiceMonitors
      serviceMonitorSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      ## PodMonitors to be selected for target discovery.
      ## If {}, select all PodMonitors
      ##
      podMonitorSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      ## Probes to be selected for target discovery.
      ## If {}, select all Probes
      ##
      probeSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      ## scrapeConfigs to be selected for target discovery.
      ## If {}, select all scrapeConfigs
      ##
      scrapeConfigSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"
      ## PrometheusRules to be selected for target discovery.
      ## If {}, select all PrometheusRules
      ##
      ruleSelector:
        matchLabels:
          plugin: "{{ $.Release.Name }}"

kubernetes-operations:
  prometheusRules:
    ruleSelector:
      - name: plugin
        value: "{{ $.Release.Name }}"


alerts:
  enabled: false
  alertmanagers:
    hosts: []
    tlsConfig:
      cert: ""
      key: ""

testFramework:
  enabled: true
  image:
    registry: ghcr.io
    repository: cloudoperators/greenhouse-extensions-integration-test
    tag: main
  imagePullPolicy: IfNotPresent
